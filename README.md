# Haufe Internship 2025 â€“ AI Code Review (Local)

Simple, local, and private codeâ€‘review tool powered by Ollama. Oneâ€‘click start on Windows, clean web UI.

## Requirements coverage

- âœ… Oneâ€‘click start on Windows (`start_all.bat`)
- âœ… Fully local LLM via Ollama (no cloud by default)
- âœ… Review any GitHub repo (full or incremental â€œChanged onlyâ€ with Files Changed summary)
- âœ… Language detection per file
- âœ… Codeâ€‘focused review with:
  - exact line numbers and corrected code snippets
  - severity (CRITICAL/HIGH/MEDIUM/LOW), effort (S/M/L), and docs notes
- âœ… Guidelines support (paste/upload) + autoâ€‘detect common config files
- âœ… Metrics (duration, batches, input/output chars, estimated tokens)
- âœ… Basic comments per repo (file/lines); quick â€œCommentâ€ from findings
- âœ… JSON cache for repo state (speeds incremental reviews)
- âœ… Validation scripts (`quick_test.py`, `test_enhancements.py`)
- âœ… Optional VS Code extension (Quick Prompt, review commands)

Missing / partial

- âŒ Exact token usage (Ollama doesnâ€™t return token counts; estimates only)
- âŒ Threaded comments anchored to findings (backend supports findingId; UI prefill only)
- âŒ Merge multiple uploaded guideline files automatically
- âŒ Authentication/roles (local singleâ€‘user only)
- âŒ Dockerization
- âŒ Mobileâ€‘optimized UI
- âŒ Cloud fallback enabled by default (you can optâ€‘in via env vars)

## How to run (Windows)

1) Install Ollama
- https://ollama.com  â†’ install
- In a terminal: `ollama pull qwen2.5-coder:7b`

2) Start everything
- Doubleâ€‘click `start_all.bat` (opens Ollama, Backend http://localhost:7070, Web http://localhost:5173)

3) Use it
- Left panel: paste code, upload/paste Guidelines, review; after repo review use â€œFindings (last review)â€ to prefill comments
- Right panel: enter repo (owner/repo or full URL), toggle Changed only, choose model, click â€œReview Repoâ€; see metrics and add comments

## Quick tests

- `quick_test.py` â€“ health + code review endpoint
- `test_enhancements.py` â€“ language detection, strict prompts, change tracking

# Haufe Internship 2025 â€“ AI Code Review (Local, Private)# ğŸ¤– AI Code Review Assistant

Turn any GitHub repo into a highâ€‘signal code review using a fully local LLM (Ollama). Oneâ€‘click start on Windows, with a modern web UI and optional VS Code commands.**Intelligent code review powered by local and cloud LLMs** - Built for privacy, speed, and accuracy.

- Local and private: runs entirely on your machine via Ollama## âœ¨ Features

- Zero cost: no cloud calls by default (OpenAI fallback optional)

- Repo review: full or incremental (changed files only), with language detection- ğŸ  **Local-first LLM** - Runs on Ollama (100% private, zero cost)

- Codeâ€‘focused prompts: mandatory line numbers, exact corrected snippets, severity, effort, docs- â˜ï¸ **Cloud fallback** - DeepSeek, OpenAI, Gemini APIs

- Guidelines: paste or upload rules (PEP8/ESLint/MD) and autoâ€‘detect common config files- ğŸ¯ **Multi-dimensional analysis** - Heuristics + language-specific + AI

- Metrics: duration, batches, character and estimated token counts- ğŸ”„ **Incremental review** - Analyze only changed code (Git diff)

- Comments: simple perâ€‘repo thread with file/line context- ğŸ› ï¸ **Auto-fix** - One-click code corrections

- ğŸš« **Pre-commit hooks** - Block bad commits automatically

- ğŸ’° **Cost tracking** - Monitor API usage and spending

## Quick start (Windows)- ğŸ¨ **Beautiful UI** - Modern webview with metrics dashboard

1. Install Ollama (if you donâ€™t have it yet)## ğŸš€ Quick Start

- Download from https://ollama.com and install

- Open a terminal and run once: `ollama pull qwen2.5-coder:7b`### 1. Install Ollama (Recommended)

2. Start everything (Ollama + Backend + Web)```bash

- Doubleâ€‘click `start_all.bat` from the repo root# Download from https://ollama.ai

- This opens three windows: Ollama service, Backend API (port 7070), Web UI (port 5173)# Pull a code model:

- A browser tab should open automatically at http://localhost:5173ollama pull deepseek-coder:6.7b

````

3) Use it

- Left panel:### 2. Build & Run

  - Paste code and click "Review" for singleâ€‘file checks

  - Upload or paste Project Guidelines```bash

  - After running a repo review (right panel), "Findings (last review)" appears with a Comment button to prefill the comment formnpm install

- Right panel:npm run compile

  - Enter repo (owner/repo or full Git URL), toggle Changed only, choose model, and click "Review Repo"# Press F5 in VS Code to launch

  - See metrics under the repo controls```

  - Add comments (file/lines can be prefilled from the left findings list)

### 3. Review Code



## Whatâ€™s included- Press `Ctrl+Shift+R` (Cmd+Shift+R on Mac)

- Or right-click â†’ "AI Code Review: Review Current File"

- backend/ â€“ Node/Express TypeScript API for local LLM via Ollama

  - `/health`, `/models`, `/api/review`, `/api/reviewRepo`## ğŸ¯ Commands

  - Comments API: `GET /api/comments?repo=...`, `POST /api/comments`

  - Caching DB (JSON) in `backend/data` (repoIndex.json, comments.json)| Command                     | Shortcut       | Description                        |

- web/ â€“ React + Vite web UI (dark, modern)| --------------------------- | -------------- | ---------------------------------- |

  - Left: code input, guidelines (upload + textarea), findings navigator| **Review Current File**     | `Ctrl+Shift+R` | Analyze open file                  |

  - Right: AI output, repo review controls, metrics, comments thread| **Review Git Changes**      | -              | Analyze staged changes only        |

- scripts/ â€“ Resilient runners for backend/web| **Install Pre-commit Hook** | -              | Block commits with critical issues |

- start_all.bat â€“ Oneâ€‘click launcher (Ollama + Backend + Web)

- quick_test.py â€“ Sanity check for health and endpoints## ğŸ”§ Configuration

- test_enhancements.py â€“ Validates language detection, prompt strictness, and incremental change display

### Environment Variables



## Repo review featuresThe extension supports multiple LLM providers:



- Language annotations per file (e.g., File: path (Language: TypeScript))```bash

- Incremental mode (changedOnly): only new/modified files; change summary included# Optional: Cloud API keys for fallback

- Batching within size limits (configurable)DEEPSEEK_API_KEY="sk-..."

- Strict review output with:OPENAI_API_KEY="sk-..."

  - Severity (CRITICAL/HIGH/MEDIUM/LOW)GEMINI_API_KEY="..."

  - File:Line```

  - Current Code + Fixed Code (exact snippets)

  - Explanation + Effort + Docs### Custom Rules



Create `code-review.config.json` in workspace root:

## Models

```json

- Default: `qwen2.5-coder:7b` (recommended for speed/quality balance){

- Use the web UI model dropdown; ensure the model is pulled in Ollama  "patterns": [

- You can add more models (e.g., `llama3.1:8b`, `deepseek-coder-v2:16b`)    {

      "name": "No console.log",

      "regex": "console\\.log",

## Developer notes      "severity": "warning",

      "message": "Remove debug logging",

- Ports: Backend 7070, Web 5173 (Vite may pick a nearby port if 5173 used)      "guideline": "Team policy",

- Windowsâ€‘first: Batch scripts included. macOS/Linux can run backend and web via npm scripts.      "tags": ["style"]

- Optional OpenAI fallback: set `ALLOW_CLOUD=true` and `OPENAI_API_KEY` in backend env to enable.    }

  ]

}

## Folder structure```



```## ğŸ“Š Supported Languages

.

â”œâ”€ backend/                # Express API (TypeScript)- **Python** - Security, PEP8, SQL injection, eval

â”‚  â”œâ”€ src/server.ts- **JavaScript** - ESLint rules, console.log, var

â”‚  â”œâ”€ data/                # JSON cache (repoIndex.json, comments.json)- **TypeScript** - Type safety, any usage

â”‚  â””â”€ package.json- **More coming soon!**

â”œâ”€ web/                    # React + Vite UI

â”‚  â”œâ”€ src/App.tsx## ğŸ¨ Demo

â”‚  â””â”€ package.json

â”œâ”€ scripts/                # Resilient runner scriptsTry the demo files:

â”œâ”€ start_all.bat           # Oneâ€‘click start (Windows)

â”œâ”€ quick_test.py           # Quick health + endpoint test```bash

â”œâ”€ test_enhancements.py    # Validation for features# Open demo/buggy_python.py

â””â”€ README.md# Issues detected: hardcoded passwords, SQL injection, eval, etc.

````

# Open demo/buggy_javascript.js

# Issues detected: var usage, console.log, eval, ==

## Useful scripts```

- quick_test.py â€“ verifies backend health and code review## ğŸ—ï¸ Architecture

- test_enhancements.py â€“ runs 3 tests (code prompts, language detection, change tracking)

- extras/QUICK_REFERENCE.md â€“ tips and hints```

Extension â†’ ReviewManager â†’ LLM Router

                              â”œâ”€ Ollama (local)

## Mermaid overview â”œâ”€ DeepSeek API

                              â”œâ”€ OpenAI GPT-4

````mermaid â””â”€ Google Gemini

flowchart LR

  A[Ollama (local models)] <-- REST --> B[Backend API (Express TS)]Analysis Pipeline:

  B <-- fetch --> C[Web UI (React + Vite)]1. Language-specific patterns

  B <- read/write -> D[(JSON Cache)]2. Security heuristics

  B <-- git clone --> E[GitHub Repo]3. Custom rules

```4. AI semantic analysis

````

## Troubleshooting## ğŸ“ˆ Performance

- Port 7070 in use: close prior backend windows or run `Stop-Process -Name node,tsx` in PowerShell, then reâ€‘run start_all.bat| Metric | Value |

- No models found: open an Ollama terminal and run `ollama pull qwen2.5-coder:7b`| -------------------- | ----------- |

- Health shows offline: ensure Ollama service window is running (`ollama serve`)| Heuristic analysis | < 100ms |

| LLM analysis (local) | 2-5s |

| LLM analysis (cloud) | 1-3s |

## License| Cost per review | $0.00-$0.01 |

This project is for the Haufe Internship 2025 showcase. If you intend to use it beyond evaluation, please add a suitable openâ€‘source license and review thirdâ€‘party model terms.## ğŸ› ï¸ Development

```bash
# Install dependencies
npm install

# Compile TypeScript
npm run compile

# Watch mode
npm run watch
```

## ğŸ“ License

MIT License

## ğŸ“ Credits

Built with:

- [VS Code Extension API](https://code.visualstudio.com/api)
- [Ollama](https://ollama.ai/)
- [DeepSeek Coder](https://github.com/deepseek-ai/DeepSeek-Coder)

---

ğŸ“– **See [HACKATHON.md](./HACKATHON.md) for complete documentation and scoring breakdown**

**Made with â¤ï¸ for better code reviews**
